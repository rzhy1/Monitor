name: Monitor URL Changes

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - '.github/workflows/monitor.yml'
      - 'url_state.json'
  schedule:
    - cron: '0 23,0-13 * * *'

permissions:
  contents: write

jobs:
  check_url_changes:
    runs-on: ubuntu-latest
    env:
      WEBHOOK_URLS: ${{ secrets.WEBHOOK_URLS }}
      CHECK_URLS: |
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2331
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2332
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3508
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3509
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3510
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3511
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3512
        https://github.com/rzhy1/aria2-static-build/blob/main/.github/workflows/build_and_release.yml
      URL_DESCRIPTIONS: |
        {"https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2331": "æ­å·åœ°é“æ‹›æ ‡è®¡åˆ’å‘å¸ƒ",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2332": "æ­å·åœ°é“æ‹›æ ‡é¢„å…¬ç¤º",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3508": "æ­å·åœ°é“ç‰¹åˆ«æé†’é¡¹ç›®",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3509": "æ­å·åœ°é“æ‹›æ ‡æ ¸å‡†é¡¹ç›®",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3510": "æ­å·åœ°é“æ‹›æ ‡æ–‡ä»¶å…¬ç¤º",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3511": "æ­å·åœ°é“ä¸­æ ‡å…¬ç¤º",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3512": "æ­å·åœ°é“ä¸­æ ‡ç»“æœ",
         "https://github.com/rzhy1/aria2-static-build/blob/main/.github/workflows/build_and_release.yml": "æµ‹è¯•ä¸“ç”¨"}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            requests \
            beautifulsoup4 \
            urllib3

      - name: Check and notify
        run: |
          python - << "EOF"
          import os
          import json
          import hashlib
          import time
          import re
          import requests
          from bs4 import BeautifulSoup, Comment
          from requests.adapters import HTTPAdapter
          from urllib3.util.retry import Retry

          # å¸¸é‡é…ç½®
          USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
          STATE_FILE = "url_state.json"

          class Monitor:
              def __init__(self):
                  self.session = self.create_session()
                  self.url_descriptions = json.loads(os.getenv("URL_DESCRIPTIONS"))
                  self.check_urls = [u.strip() for u in os.getenv("CHECK_URLS").splitlines() if u.strip()]
                  self.base_url = "https://ztb.cxjw.hangzhou.gov.cn:8092"

              def create_session(self):
                  """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯"""
                  session = requests.Session()
                  retry = Retry(
                      total=3,
                      backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504],
                      allowed_methods=['GET']
                  )
                  adapter = HTTPAdapter(max_retries=retry)
                  session.mount('https', adapter)
                  session.mount('http', adapter)
                  return session

              def extract_project_titles(self, html, current_url):
                  """æå–é¡¹ç›®æ ‡é¢˜å’Œé“¾æ¥ - ä½¿ç”¨æ›´çµæ´»çš„é€‰æ‹©å™¨"""
                  soup = BeautifulSoup(html, 'html.parser')
                  titles_with_links = []
                  
                  # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
                  selectors = [
                      'div.menu.menu_time',
                      'div.list > div',  # ç›´æ¥è·å–åˆ—è¡¨ä¸‹çš„div
                      '.listWrap .list > div'  # æ›´å…·ä½“çš„é€‰æ‹©å™¨
                  ]
                  
                  menu_divs = []
                  for selector in selectors:
                      menu_divs = soup.select(selector)
                      if menu_divs:
                          print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°äº† {len(menu_divs)} ä¸ªå…ƒç´ ")
                          break
                  
                  if not menu_divs:
                      print("è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•é¡¹ç›®å…ƒç´ ï¼Œå°è¯•å¤‡ç”¨é€‰æ‹©å™¨")
                      # å¤‡ç”¨æ–¹æ¡ˆ: æŸ¥æ‰¾åŒ…å«onclickçš„div
                      menu_divs = soup.find_all('div', onclick=lambda x: x and 'window.open' in x)
                      print(f"å¤‡ç”¨é€‰æ‹©å™¨æ‰¾åˆ°äº† {len(menu_divs)} ä¸ªå…ƒç´ ")
                  
                  print(f"æ€»å…±æ‰¾åˆ° {len(menu_divs)} ä¸ªå¯èƒ½çš„é¡¹ç›®å…ƒç´ ")
                  
                  for i, div in enumerate(menu_divs):
                      # ä¼˜å…ˆæŸ¥æ‰¾h3ï¼Œå¦‚æœæ²¡æœ‰åˆ™æŸ¥æ‰¾å…¶ä»–å¯èƒ½åŒ…å«æ ‡é¢˜çš„æ ‡ç­¾
                      title_element = div.find('h3') or div.find('a') or div
                      title_text = title_element.get_text(strip=True)
                      
                      # è¿‡æ»¤æ‰ç©ºæ ‡é¢˜æˆ–éé¡¹ç›®æ ‡é¢˜
                      if not title_text or len(title_text) < 5:
                          continue
                          
                      print(f"é¡¹ç›® {i+1}: {title_text}")
                      
                      # å°è¯•ä»onclickå±æ€§ä¸­æå–é“¾æ¥
                      onclick = div.get('onclick', '')
                      detail_url = None
                      
                      if onclick and 'window.open' in onclick:
                          # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–URL
                          match = re.search(r"window\.open\s*\(\s*['\"]([^'\"]+)['\"]", onclick)
                          if match:
                              detail_path = match.group(1)
                              # æ„å»ºå®Œæ•´URL
                              if detail_path.startswith('/'):
                                  detail_url = f"{self.base_url}{detail_path}"
                              else:
                                  detail_url = detail_path
                      
                      # å¦‚æœæ²¡æœ‰æå–åˆ°è¯¦ç»†é“¾æ¥ï¼Œä½¿ç”¨å½“å‰é¡µé¢URL
                      if not detail_url:
                          detail_url = current_url
                      
                      titles_with_links.append((title_text, detail_url))
                  
                  print(f"æˆåŠŸæå– {len(titles_with_links)} ä¸ªé¡¹ç›®æ ‡é¢˜")
                  return titles_with_links

              def get_content_hash(self, url):
                  """è·å–å†…å®¹å“ˆå¸Œå€¼ - ä¸“æ³¨äºé¡¹ç›®å†…å®¹æœ¬èº«ï¼Œå¿½ç•¥é¡ºåº"""
                  try:
                      response = self.session.get(
                          url,
                          headers={'User-Agent': USER_AGENT, 'Cache-Control': 'no-cache'},
                          timeout=10
                      )
                      response.raise_for_status()
                      
                      # æå–æ‰€æœ‰é¡¹ç›®æ ‡é¢˜å’Œé“¾æ¥ï¼ŒåŸºäºè¿™äº›å†…å®¹è®¡ç®—å“ˆå¸Œ
                      titles_with_links = self.extract_project_titles(response.text, url)
                      
                      # å¯¹é¡¹ç›®æ ‡é¢˜è¿›è¡Œæ’åºï¼Œç¡®ä¿é¡ºåºä¸å½±å“å“ˆå¸Œå€¼
                      sorted_titles = sorted([title for title, link in titles_with_links])
                      
                      # åªä½¿ç”¨æ’åºåçš„é¡¹ç›®æ ‡é¢˜æ¥è®¡ç®—å“ˆå¸Œï¼Œå¿½ç•¥é“¾æ¥å’Œé¡ºåº
                      content_for_hash = "\n".join(sorted_titles)
                      
                      print(f"ä½¿ç”¨æ’åºåçš„é¡¹ç›®æ ‡é¢˜è®¡ç®—å“ˆå¸Œï¼Œå†…å®¹é•¿åº¦: {len(content_for_hash)}")
                      return hashlib.sha256(content_for_hash.encode()).hexdigest(), response.text
                      
                  except Exception as e:
                      print(f"[ERROR] æ£€æŸ¥ {url} å¤±è´¥: {str(e)}")
                      return None, None

              def load_state(self):
                  """åŠ è½½å†å²çŠ¶æ€"""
                  try:
                      with open(STATE_FILE, 'r') as f:
                          state = json.load(f)
                  except (FileNotFoundError, json.JSONDecodeError):
                      state = {}
                  
                  # ç¡®ä¿çŠ¶æ€æ–‡ä»¶æœ‰æ­£ç¡®çš„ç»“æ„
                  for url in self.check_urls:
                      if url not in state:
                          state[url] = {"hash": None, "titles": []}
                      elif isinstance(state[url], str):  # å…¼å®¹æ—§ç‰ˆæœ¬çš„çŠ¶æ€æ–‡ä»¶
                          state[url] = {"hash": state[url], "titles": []}
                  
                  return state

              def save_state(self, state):
                  """ä¿å­˜å½“å‰çŠ¶æ€"""
                  try:
                      with open(STATE_FILE, 'w') as f:
                          json.dump(state, f, indent=2)
                  except IOError as e:
                      print(f"[ERROR] ä¿å­˜çŠ¶æ€å¤±è´¥: {str(e)}")

              def get_new_titles(self, current_titles, previous_titles):
                  """æ‰¾å‡ºæ–°å¢çš„æ ‡é¢˜"""
                  # å°†ä¹‹å‰çš„æ ‡é¢˜è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
                  previous_set = set(previous_titles)
                  # æ‰¾å‡ºå½“å‰æ ‡é¢˜ä¸­ä¸åœ¨ä¹‹å‰æ ‡é¢˜é›†åˆä¸­çš„é¡¹ç›®
                  new_titles = [title for title in current_titles if title[0] not in previous_set]
                  
                  print(f"ä¹‹å‰æœ‰ {len(previous_set)} ä¸ªé¡¹ç›®ï¼Œå½“å‰æœ‰ {len(current_titles)} ä¸ªé¡¹ç›®ï¼Œæ–°å¢ {len(new_titles)} ä¸ªé¡¹ç›®")
                  return new_titles

              def send_notifications(self, changed_urls, new_titles_map):
                  """å‘é€é€šçŸ¥"""
                  # å‡†å¤‡æ¶ˆæ¯å†…å®¹
                  markdown_msgs = []  # æ”¹ä¸ºåˆ—è¡¨ï¼Œå­˜å‚¨å¤šæ¡æ¶ˆæ¯
                  
                  for url in changed_urls:
                      desc = self.url_descriptions.get(url, "æœªçŸ¥æ›´æ–°")
                      new_titles = new_titles_map.get(url, [])
                      
                      if new_titles:
                          # ä¸ºæ¯ä¸ªæ–°å¢æ ‡é¢˜åˆ›å»ºå¸¦é“¾æ¥çš„æ ¼å¼
                          title_links = []
                          for title, link in new_titles:
                              title_links.append(f"[{title}]({link})")
                          
                          # å°†é¡¹ç›®åˆ—è¡¨çš„æ ‡è®°æ”¹ä¸ºçº¢è‰²åœ†ç‚¹
                          title_list = "\n".join([f"ğŸ”´ {link}" for link in title_links])
                          
                          # æ„å»ºå•æ¡æ¶ˆæ¯
                          message = f"**{desc}**\n{title_list}\n[ğŸ”µæŸ¥çœ‹è¯¦æƒ…]({url})"
                          
                          # æ£€æŸ¥æ¶ˆæ¯é•¿åº¦
                          if len(message) > 4000:  # ç•™å‡ºä¸€äº›ç©ºé—´ç»™æ¶ˆæ¯å¤´
                              # å¦‚æœæ¶ˆæ¯è¿‡é•¿ï¼Œåˆ†å‰²æˆå¤šæ¡
                              parts = self.split_long_message(message)
                              markdown_msgs.extend(parts)
                          else:
                              markdown_msgs.append(message)
                      else:
                          # é¡µé¢å†…å®¹æœ‰å˜åŒ–ä½†æ²¡æœ‰æ–°å¢é¡¹ç›®
                          message = f"**{desc}**\né¡µé¢å†…å®¹æœ‰å˜åŒ–ï¼Œä½†æ²¡æœ‰æ£€æµ‹åˆ°æ–°å¢é¡¹ç›®\n[ğŸ”µæŸ¥çœ‹è¯¦æƒ…]({url})"
                          markdown_msgs.append(message)
                  
                  # ä¼ä¸šå¾®ä¿¡é€šçŸ¥
                  webhook_urls = os.getenv("WEBHOOK_URLS")
                  if webhook_urls:
                      try:
                          webhooks = json.loads(webhook_urls)
                          self._send_wechatwork(markdown_msgs, webhooks)
                      except json.JSONDecodeError:
                          print("WEBHOOK_URLS æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æä¸ºJSON")
                  else:
                      print("æœªè®¾ç½® WEBHOOK_URLSï¼Œè·³è¿‡é€šçŸ¥å‘é€")

              def split_long_message(self, message, max_length=4000):
                  """åˆ†å‰²è¿‡é•¿çš„æ¶ˆæ¯"""
                  parts = []
                  while len(message) > max_length:
                      # æ‰¾åˆ°æœ€åä¸€ä¸ªæ¢è¡Œç¬¦ä½ç½®ï¼Œå°½é‡åœ¨å®Œæ•´è¡Œå¤„åˆ†å‰²
                      split_pos = message.rfind('\n', 0, max_length)
                      if split_pos == -1:
                          # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¢è¡Œç¬¦ï¼Œå°±åœ¨æœ€å¤§é•¿åº¦å¤„åˆ†å‰²
                          split_pos = max_length
                      
                      parts.append(message[:split_pos])
                      message = message[split_pos:]
                  
                  if message:
                      parts.append(message)
                  
                  return parts

              def _send_wechatwork(self, items, webhooks):
                  """å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥"""
                  for webhook in webhooks:
                      try:
                          print(f"å‡†å¤‡å‘é€ {len(items)} æ¡æ¶ˆæ¯åˆ° {webhook.get('note', 'æœªçŸ¥webhook')}")
                          
                          # é€æ¡å‘é€æ¶ˆæ¯ï¼Œé¿å…ä¸€æ¬¡æ€§å‘é€è¿‡å¤š
                          for i, item in enumerate(items):
                              print(f"å‘é€ç¬¬ {i+1}/{len(items)} æ¡æ¶ˆæ¯")
                              
                              data = {
                                  "msgtype": "markdown",
                                  "markdown": {
                                      "content": item
                                  }
                              }
                              
                              resp = self.session.post(
                                  webhook['url'],
                                  json=data,
                                  timeout=15
                              )
                              
                              print(f"å“åº”çŠ¶æ€ç : {resp.status_code}")
                              if resp.status_code == 200:
                                  result = resp.json()
                                  if result.get('errcode') == 0:
                                      print(f"âœ… ç¬¬ {i+1} æ¡æ¶ˆæ¯å‘é€æˆåŠŸ")
                                  else:
                                      print(f"âŒ ç¬¬ {i+1} æ¡æ¶ˆæ¯å‘é€å¤±è´¥: {result.get('errmsg')}")
                              else:
                                  print(f"âŒ ç¬¬ {i+1} æ¡æ¶ˆæ¯å‘é€å¤±è´¥: {resp.status_code}")
                              
                              # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
                              time.sleep(1)
                              
                      except Exception as e:
                          print(f"[ERROR] ä¼ä¸šå¾®ä¿¡é€šçŸ¥å¼‚å¸¸: {str(e)}")
                          import traceback
                          traceback.print_exc()

              def run(self):
                  """ä¸»æ‰§è¡Œæµç¨‹"""
                  previous_state = self.load_state()
                  current_state = {}
                  changed_urls = []
                  new_titles_map = {}  # å­˜å‚¨æ¯ä¸ªURLå¯¹åº”çš„æ–°å¢é¡¹ç›®æ ‡é¢˜å’Œé“¾æ¥

                  for url in self.check_urls:
                      print(f"\næ£€æŸ¥ URL: {url}")
                      result = self.get_content_hash(url)
                      if result[0] is not None:
                          current_hash, html_content = result
                          # æå–å½“å‰æ‰€æœ‰æ ‡é¢˜
                          current_titles = self.extract_project_titles(html_content, url)
                          
                          # è·å–ä¹‹å‰çš„æ ‡é¢˜
                          previous_titles = previous_state.get(url, {}).get("titles", [])
                          
                          # æ‰¾å‡ºæ–°å¢çš„æ ‡é¢˜
                          new_titles = self.get_new_titles(current_titles, previous_titles)
                          
                          # å‡†å¤‡å½“å‰çŠ¶æ€å­˜å‚¨
                          current_state[url] = {
                              "hash": current_hash,
                              "titles": [title[0] for title in current_titles]  # åªå­˜å‚¨æ ‡é¢˜æ–‡æœ¬ï¼Œä¸å­˜å‚¨é“¾æ¥
                          }
                          
                          # å¦‚æœæœ‰æ–°å¢æ ‡é¢˜æˆ–å“ˆå¸Œå€¼å˜åŒ–
                          previous_hash = previous_state.get(url, {}).get("hash")
                          if new_titles or previous_hash != current_hash:
                              changed_urls.append(url)
                              new_titles_map[url] = new_titles
                              print(f"URL {url} æ£€æµ‹åˆ°å˜åŒ–: æ–°å¢é¡¹ç›®={len(new_titles)}ä¸ª, å“ˆå¸Œå˜åŒ–={previous_hash != current_hash}")
                          else:
                              print(f"URL {url} æœªæ£€æµ‹åˆ°å˜åŒ–")
                          
                      time.sleep(1)  # åŸºç¡€è¯·æ±‚é—´éš”

                  if changed_urls:
                      print(f"\næ£€æµ‹åˆ° {len(changed_urls)} é¡¹æ›´æ–°")
                      for url in changed_urls:
                          desc = self.url_descriptions.get(url, "æœªçŸ¥æ›´æ–°")
                          new_titles = new_titles_map.get(url, [])
                          print(f"â­ {desc}: {url}")
                          if new_titles:
                              for title, link in new_titles:
                                  print(f"  æ–°å¢é¡¹ç›®: {title} -> {link}")
                          else:
                              print("  é¡µé¢å†…å®¹æœ‰å˜åŒ–ï¼Œä½†æ²¡æœ‰æ£€æµ‹åˆ°æ–°å¢é¡¹ç›®")
                      
                      self.send_notifications(changed_urls, new_titles_map)
                      self.save_state(current_state)
                  else:
                      print("æœªæ£€æµ‹åˆ°ä»»ä½•å˜åŒ–")

          if __name__ == "__main__":
              Monitor().run()
          EOF

      - name: Commit and push state file
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          if git diff --quiet url_state.json; then
            echo "No changes detected in url_state.json."
          else
            echo "Changes detected in url_state.json, preparing to commit..."
            git add url_state.json
            git commit -m "Update URL states [skip ci]" || echo "No changes to commit."
            git push origin HEAD || echo "Push failed, please check permissions or network issues."
            echo "çŠ¶æ€æ–‡ä»¶å·²æ›´æ–°å¹¶æäº¤"
          fi
