name: Monitor URL Changes

on:
  workflow_dispatch:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 23,0-13 * * *'

permissions:
  contents: write

jobs:
  check_url_changes:
    runs-on: ubuntu-latest
    env:
      WEBHOOK_URLS: ${{ secrets.WEBHOOK_URLS || '[]' }}
      CHECK_URLS: |
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E8%BD%A6%E8%BE%86%E6%AE%B5&status=5&channelId=2331
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2331
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2332
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3508
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3509
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3510
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3511
        https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3512
        https://github.com/rzhy1/aria2-static-build/blob/main/.github/workflows/build_and_release.yml
      URL_DESCRIPTIONS: |
        {"https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E8%BD%A6%E8%BE%86%E6%AE%B5&status=5&channelId=2331": "è½¦è¾†æ®µ",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2331": "æ­å·åœ°é“æ‹›æ ‡è®¡åˆ’å‘å¸ƒ",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=2332": "æ­å·åœ°é“æ‹›æ ‡é¢„å…¬ç¤º",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3508": "æ­å·åœ°é“ç‰¹åˆ«æé†’é¡¹ç›®",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3509": "æ­å·åœ°é“æ‹›æ ‡æ ¸å‡†é¡¹ç›®",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3510": "æ­å·åœ°é“æ‹›æ ‡æ–‡ä»¶å…¬ç¤º",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3511": "æ­å·åœ°é“ä¸­æ ‡å…¬ç¤º",
         "https://ztb.cxjw.hangzhou.gov.cn:8092/search/queryContents.jhtml?titleOrCode=%E5%9F%8E%E5%B8%82%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A&status=5&channelId=3512": "æ­å·åœ°é“ä¸­æ ‡ç»“æœ",
         "https://github.com/rzhy1/aria2-static-build/blob/main/.github/workflows/build_and_release.yml": "æµ‹è¯•ä¸“ç”¨"}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            requests==2.31.0 \
            beautifulsoup4==4.12.2 \
            urllib3==2.0.7

      - name: Check and notify
        run: |
          python - << "EOF"
          import os
          import json
          import hashlib
          import time
          import re
          import requests
          from bs4 import BeautifulSoup, Comment
          from requests.adapters import HTTPAdapter
          from urllib3.util.retry import Retry

          # å¸¸é‡é…ç½®
          USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
          STATE_FILE = "url_state.json"
          CLEAN_TAGS = ['script', 'style', 'nav', 'footer', 'aside', 'header']
          CONTENT_SELECTORS = [
              ('div', {'class': 'menu menu_time'}),
              ('div', {'id': 'main-content'}),
              ('section', {'role': 'main'}),
              ('article', {}),      # æ–°å¢é€šç”¨æ–‡ç« åŒºåŸŸ
              ('main', {}),         # æ–°å¢ä¸»å†…å®¹åŒº
              ('div', {'class': 'content'})  # æ–°å¢é€šç”¨å†…å®¹div
          ]

          class Monitor:
              def __init__(self):
                  self.session = self.create_session()
                  self.url_descriptions = json.loads(os.getenv("URL_DESCRIPTIONS"))
                  self.check_urls = [u.strip() for u in os.getenv("CHECK_URLS").splitlines() if u.strip()]
                  self.base_url = "https://ztb.cxjw.hangzhou.gov.cn:8092"

              def create_session(self):
                  """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯"""
                  session = requests.Session()
                  retry = Retry(
                      total=3,
                      backoff_factor=1,
                      status_forcelist=[429, 500, 502, 503, 504],
                      allowed_methods=['GET']
                  )
                  adapter = HTTPAdapter(max_retries=retry)
                  session.mount('https', adapter)
                  session.mount('http', adapter)
                  return session

              def clean_content(self, html):
                  """æ¸…ç†HTMLå†…å®¹"""
                  soup = BeautifulSoup(html, 'html.parser')
                  
                  # ç§»é™¤å¹²æ‰°å…ƒç´ 
                  for tag in CLEAN_TAGS:
                      for element in soup.find_all(tag):
                          element.decompose()
                  
                  # ç§»é™¤æ³¨é‡Š
                  for comment in soup.find_all(string=lambda t: isinstance(t, Comment)):
                      comment.extract()

                  # å®šä½ä¸»è¦å†…å®¹
                  main_content = None
                  for selector in CONTENT_SELECTORS:
                      main_content = soup.find(*selector)
                      if main_content: break
                  return main_content or soup

              def extract_project_titles(self, html, current_url):
                  """æå–é¡¹ç›®æ ‡é¢˜å’Œé“¾æ¥"""
                  soup = BeautifulSoup(html, 'html.parser')
                  titles_with_links = []
                  
                  # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é¡¹ç›®æ ‡é¢˜çš„div
                  for div in soup.find_all('div', class_='menu menu_time'):
                      h3 = div.find('h3')
                      if h3:
                          # æå–çº¯æ–‡æœ¬å†…å®¹ï¼Œå»é™¤HTMLæ ‡ç­¾
                          title_text = h3.get_text(strip=True)
                          
                          # å°è¯•ä»onclickå±æ€§ä¸­æå–é“¾æ¥
                          onclick = div.get('onclick', '')
                          detail_url = None
                          
                          if onclick and 'window.open' in onclick:
                              # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–URL
                              match = re.search(r"window\.open\('([^']+)'\)", onclick)
                              if match:
                                  detail_path = match.group(1)
                                  # æ„å»ºå®Œæ•´URL
                                  if detail_path.startswith('/'):
                                      detail_url = f"{self.base_url}{detail_path}"
                                  else:
                                      detail_url = detail_path
                          
                          # å¦‚æœæ²¡æœ‰æå–åˆ°è¯¦ç»†é“¾æ¥ï¼Œä½¿ç”¨å½“å‰é¡µé¢URL
                          if not detail_url:
                              detail_url = current_url
                          
                          titles_with_links.append((title_text, detail_url))
                  
                  return titles_with_links

              def get_content_hash(self, url):
                  """è·å–å†…å®¹å“ˆå¸Œå€¼"""
                  try:
                      response = self.session.get(
                          url,
                          headers={'User-Agent': USER_AGENT, 'Cache-Control': 'no-cache'},
                          timeout=10
                      )
                      response.raise_for_status()
                      
                      cleaned = self.clean_content(response.text)
                      text = cleaned.get_text('\n', strip=True)
                      return hashlib.sha256(text.encode()).hexdigest(), response.text
                  except Exception as e:
                      print(f"[ERROR] æ£€æŸ¥ {url} å¤±è´¥: {str(e)}")
                      return None, None

              def load_state(self):
                  """åŠ è½½å†å²çŠ¶æ€"""
                  try:
                      with open(STATE_FILE, 'r') as f:
                          state = json.load(f)
                  except (FileNotFoundError, json.JSONDecodeError):
                      state = {}
                  
                  # ç¡®ä¿çŠ¶æ€æ–‡ä»¶æœ‰æ­£ç¡®çš„ç»“æ„
                  for url in self.check_urls:
                      if url not in state:
                          state[url] = {"hash": None, "titles": []}
                      elif isinstance(state[url], str):  # å…¼å®¹æ—§ç‰ˆæœ¬çš„çŠ¶æ€æ–‡ä»¶
                          state[url] = {"hash": state[url], "titles": []}
                  
                  return state

              def save_state(self, state):
                  """ä¿å­˜å½“å‰çŠ¶æ€"""
                  try:
                      with open(STATE_FILE, 'w') as f:
                          json.dump(state, f, indent=2)
                  except IOError as e:
                      print(f"[ERROR] ä¿å­˜çŠ¶æ€å¤±è´¥: {str(e)}")

              def get_new_titles(self, current_titles, previous_titles):
                  """æ‰¾å‡ºæ–°å¢çš„æ ‡é¢˜"""
                  # å°†ä¹‹å‰çš„æ ‡é¢˜è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
                  previous_set = set(previous_titles)
                  # æ‰¾å‡ºå½“å‰æ ‡é¢˜ä¸­ä¸åœ¨ä¹‹å‰æ ‡é¢˜é›†åˆä¸­çš„é¡¹ç›®
                  return [title for title in current_titles if title[0] not in previous_set]

              def send_notifications(self, changed_urls, new_titles_map):
                  """å‘é€é€šçŸ¥"""
                  # å‡†å¤‡æ¶ˆæ¯å†…å®¹
                  markdown_msg = []
                  for url in changed_urls:
                      desc = self.url_descriptions.get(url, "æœªçŸ¥æ›´æ–°")
                      new_titles = new_titles_map.get(url, [])
                      
                      if new_titles:
                          # ä¸ºæ¯ä¸ªæ–°å¢æ ‡é¢˜åˆ›å»ºå¸¦é“¾æ¥çš„æ ¼å¼
                          title_links = []
                          for title, link in new_titles:
                              title_links.append(f"[{title}]({link})")
                          
                          # å°†é¡¹ç›®åˆ—è¡¨çš„æ ‡è®°æ”¹ä¸ºçº¢è‰²åœ†ç‚¹
                          title_list = "\n".join([f"ğŸ”´ {link}" for link in title_links])
                          # ä¿ç•™æŸ¥çœ‹è¯¦æƒ…é“¾æ¥ï¼ŒæŒ‡å‘åŸç›‘æ§é¡µé¢ï¼Œå¹¶æ·»åŠ è“è‰²åœ†ç‚¹
                          markdown_msg.append(f"**{desc}**\n{title_list}\n[ğŸ”µæŸ¥çœ‹è¯¦æƒ…]({url})")
                      else:
                          # é¡µé¢å†…å®¹æœ‰å˜åŒ–ä½†æ²¡æœ‰æ–°å¢é¡¹ç›®
                          markdown_msg.append(f"**{desc}**\né¡µé¢å†…å®¹æœ‰å˜åŒ–ï¼Œä½†æ²¡æœ‰æ£€æµ‹åˆ°æ–°å¢é¡¹ç›®\n[ğŸ”µæŸ¥çœ‹è¯¦æƒ…]({url})")
                  
                  # ä¼ä¸šå¾®ä¿¡é€šçŸ¥
                  if webhooks := json.loads(os.getenv("WEBHOOK_URLS", "[]")):
                      self._send_wechatwork(markdown_msg, webhooks)

              def _send_wechatwork(self, items, webhooks):
                  """å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥"""
                  for webhook in webhooks:
                      try:
                          resp = self.session.post(
                              webhook['url'],
                              json={
                                  "msgtype": "markdown",
                                  "markdown": {
                                      "content": "### æ›´æ–°é€šçŸ¥\n" + "\n\n".join(items)
                                  }
                              },
                              timeout=15
                          )
                          if resp.status_code == 200:
                              print(f"ä¼ä¸šå¾®ä¿¡é€šçŸ¥æˆåŠŸ: {webhook.get('note')}")
                          else:
                              print(f"[ERROR] ä¼ä¸šå¾®ä¿¡é€šçŸ¥å¤±è´¥: {resp.status_code}")
                      except Exception as e:
                          print(f"[ERROR] ä¼ä¸šå¾®ä¿¡é€šçŸ¥å¼‚å¸¸: {str(e)}")

              def run(self):
                  """ä¸»æ‰§è¡Œæµç¨‹"""
                  previous_state = self.load_state()
                  current_state = {}
                  changed_urls = []
                  new_titles_map = {}  # å­˜å‚¨æ¯ä¸ªURLå¯¹åº”çš„æ–°å¢é¡¹ç›®æ ‡é¢˜å’Œé“¾æ¥

                  for url in self.check_urls:
                      result = self.get_content_hash(url)
                      if result[0] is not None:
                          current_hash, html_content = result
                          # æå–å½“å‰æ‰€æœ‰æ ‡é¢˜
                          current_titles = self.extract_project_titles(html_content, url)
                          
                          # è·å–ä¹‹å‰çš„æ ‡é¢˜
                          previous_titles = previous_state.get(url, {}).get("titles", [])
                          
                          # æ‰¾å‡ºæ–°å¢çš„æ ‡é¢˜
                          new_titles = self.get_new_titles(current_titles, previous_titles)
                          
                          # å‡†å¤‡å½“å‰çŠ¶æ€å­˜å‚¨
                          current_state[url] = {
                              "hash": current_hash,
                              "titles": [title[0] for title in current_titles]  # åªå­˜å‚¨æ ‡é¢˜æ–‡æœ¬ï¼Œä¸å­˜å‚¨é“¾æ¥
                          }
                          
                          # å¦‚æœæœ‰æ–°å¢æ ‡é¢˜æˆ–å“ˆå¸Œå€¼å˜åŒ–
                          if new_titles or previous_state.get(url, {}).get("hash") != current_hash:
                              changed_urls.append(url)
                              new_titles_map[url] = new_titles
                          
                      time.sleep(1)  # åŸºç¡€è¯·æ±‚é—´éš”

                  if changed_urls:
                      print(f"æ£€æµ‹åˆ° {len(changed_urls)} é¡¹æ›´æ–°")
                      for url in changed_urls:
                          desc = self.url_descriptions.get(url, "æœªçŸ¥æ›´æ–°")
                          new_titles = new_titles_map.get(url, [])
                          print(f"â­ {desc}: {url}")
                          if new_titles:
                              for title, link in new_titles:
                                  print(f"  æ–°å¢é¡¹ç›®: {title} -> {link}")
                          else:
                              print("  é¡µé¢å†…å®¹æœ‰å˜åŒ–ï¼Œä½†æ²¡æœ‰æ£€æµ‹åˆ°æ–°å¢é¡¹ç›®")
                      
                      self.send_notifications(changed_urls, new_titles_map)
                      self.save_state(current_state)
                  else:
                      print("æœªæ£€æµ‹åˆ°å˜åŒ–")

          if __name__ == "__main__":
              Monitor().run()
          EOF

      - name: Commit and push state file
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          if git diff --quiet url_state.json; then
            echo "No changes detected in url_state.json."
          else
            echo "Changes detected in url_state.json, preparing to commit..."
            git add url_state.json
            git commit -m "Update URL states [skip ci]" || echo "No changes to commit."
            git push origin HEAD || echo "Push failed, please check permissions or network issues."
            echo "çŠ¶æ€æ–‡ä»¶å·²æ›´æ–°å¹¶æäº¤"
          fi
